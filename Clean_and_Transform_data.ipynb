{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "from sklearn.cluster import MiniBatchKMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Agrupa por columnas, saca la media de la columna target para cada grupo y ordena dichos grupos segun la media obtenida\n",
    "def map_order_col_by_target(X,col,target):\n",
    "    \n",
    "    grouped_by_col=X.groupby([col])[target].mean().sort_values()\n",
    "    mapping={}\n",
    "    for i, index in enumerate(grouped_by_col.index):\n",
    "        mapping[index]=i\n",
    "\n",
    "    return mapping\n",
    "\n",
    "#Cantidad de valores faltantes de una o mas (o todas las) columnas\n",
    "def n_missings(X,cols=''):\n",
    "    if cols:\n",
    "        return X[cols].isnull().sum() #solo algunas columnas\n",
    "    else: \n",
    "        return X.isnull().sum() #todas las columnas\n",
    "\n",
    "    \n",
    "#GENERA LOS VALORES FALTANTES DE FORMA ALEATORIA PONDERADA POR LAS FRECUENCIAS RELATIVAS\n",
    "#ES DECIR, CADA VALOR FALTANTE ES MAS PROBABLE QUE SE RELLENE CON LOS MAS FRECUENTES, \n",
    "#ASI SE MANTIENE LA DISTRIBUCION DE PROBABILIDAD ORIGINAL INTACTA    \n",
    "def rellenar_faltantes_random_ponderado(X,col):\n",
    "    #OBTENEMOS LA FRECUENCIA RELATIVA DE CADA VALOR\n",
    "    val_count=X[col].value_counts()/X.shape[0]\n",
    "    \n",
    "    #A LA QUE TIENE MAS FRECUENCIA LE AÑADIMOS LO QUE FALTA PARA LLEGAR A 1(EN LA DIVISION SE PIERDEN DECIMALES Y FALTABA 0.0001 O ASI)\n",
    "    val_count.iloc[0]+=1-val_count.values.sum()\n",
    "    \n",
    "    n_col_nulls=X[col].isnull().sum()\n",
    "    return np.random.choice(val_count.index,n_col_nulls,p=val_count.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_datasets():\n",
    "        #CARGAMOS LOS DATOS\n",
    "\n",
    "    traindata=pd.read_csv('Dataset_Salesforce_Predictive_Modelling_TRAIN.txt')#reading the data\n",
    "    testdata=pd.read_csv('Dataset_Salesforce_Predictive_Modelling_TEST.txt')\n",
    "    #print('Number of rows and columns of the training set: ',traindata.shape)\n",
    "    #print('Number of rows and columns of the test set: ',testdata.shape)\n",
    "    #VARIABLES DE CADA TIPO\n",
    "    list_columns_test=list(testdata.columns.values)\n",
    "    #Ver el nombre de las columnas para saber cuales sumar (buscar la mayor correlación)\n",
    "    list_columns=list(traindata.columns.values)\n",
    "    Names_Imp_cons=list_columns[0:17]\n",
    "    #Importe de consumos habituales del cliente en base a sus operaciones con tarjetas y domiciliaciones más comunes\n",
    "    traindata['Imp_cons_total'] = traindata[Names_Imp_cons].sum(axis=1)\n",
    "    #Importe de los saldos de los distintos productos financieros.\n",
    "    traindata['Imp_cons_total_2']=traindata['Imp_cons_total']**2\n",
    "    traindata['Imp_cons_total_3']=traindata['Imp_cons_total']**3\n",
    "    Names_Imp_Sal=list_columns[17:38]\n",
    "    traindata['Imp_sal_total'] = traindata[Names_Imp_Sal].sum(axis=1)\n",
    "    #Tenencia de los distintos productos financieros. Son indices entre 0 y 2 por lo que el sumatorio también valdra\n",
    "    #ya que cuanta más puntuación saque el sumatorio más productos compra\n",
    "    traindata['Imp_sal_total_2']=traindata['Imp_sal_total']**2\n",
    "    traindata['Imp_sal_total_3']=traindata['Imp_sal_total']**3\n",
    "    Names_Ind_Prod=list_columns[38:62]\n",
    "    traindata['Ind_prod_total'] = traindata[Names_Ind_Prod].sum(axis=1)\n",
    "    #Número de operaciones a través de los distintos productos financieros.\n",
    "    traindata['Ind_prod_total_2'] = traindata['Ind_prod_total']**2\n",
    "    traindata['Ind_prod_total_3'] = traindata['Ind_prod_total']**3\n",
    "    Names_Num_Oper=list_columns[62:82]\n",
    "    traindata['Num_Oper_total'] = traindata[Names_Num_Oper].sum(axis=1)\n",
    "\n",
    "    traindata['Num_Oper_total_2'] = traindata['Num_Oper_total']**2\n",
    "    traindata['Num_Oper_total_3'] = traindata['Num_Oper_total']**3\n",
    "    #datasetfinal\n",
    "    traindata['Relacion']=((traindata.Imp_sal_total+traindata.Ind_prod_total)*traindata.Num_Oper_total)-traindata.Imp_cons_total\n",
    "    #df = data[['Relacion','Imp_cons_total', 'Imp_sal_total', 'Ind_prod_total','Num_Oper_total','Socio_Demo_01','Socio_Demo_02','Socio_Demo_03','Socio_Demo_04','Socio_Demo_05','Poder_Adquisitivo']].copy()\n",
    "\n",
    "    traindata['Relacion_2'] = traindata['Relacion']**2\n",
    "    traindata['Relacion_3'] = traindata['Relacion']**3\n",
    "    #hacemos lo mismo con lo de test (la lista que contiene nombres nos sirve la misma)\n",
    "\n",
    "    testdata['Imp_cons_total'] = testdata[Names_Imp_cons].sum(axis=1)\n",
    "    testdata['Imp_cons_total_2'] = testdata['Imp_cons_total']**2\n",
    "    testdata['Imp_cons_total_3'] = testdata['Imp_cons_total']**3\n",
    "    testdata['Imp_sal_total'] = testdata[Names_Imp_Sal].sum(axis=1)\n",
    "    testdata['Imp_sal_total_2'] = testdata['Imp_sal_total']**2\n",
    "    testdata['Imp_sal_total_3'] = testdata['Imp_sal_total']**3\n",
    "    testdata['Ind_prod_total'] = testdata[Names_Ind_Prod].sum(axis=1)\n",
    "    testdata['Ind_prod_total_2'] = testdata['Ind_prod_total']**2\n",
    "    testdata['Ind_prod_total_3'] = testdata['Ind_prod_total']**3\n",
    "    testdata['Num_Oper_total'] = testdata[Names_Num_Oper].sum(axis=1)\n",
    "    testdata['Num_Oper_total_2'] = testdata['Num_Oper_total']**2\n",
    "    testdata['Num_Oper_total_3'] = testdata['Num_Oper_total']**3\n",
    "    testdata['Relacion']=((testdata.Imp_sal_total+testdata.Ind_prod_total)*testdata.Num_Oper_total)-testdata.Imp_cons_total\n",
    "    testdata['Relacion_2']= testdata['Relacion']**2\n",
    "    testdata['Relacion_3']= testdata['Relacion']**3\n",
    "    traindata.dtypes.value_counts()\n",
    "\n",
    "    numeric_cols=traindata.select_dtypes(include=[np.number]).columns#select only numerical\n",
    "    nominal_cols=traindata.select_dtypes(exclude=[np.number]).columns#select only non numerical\n",
    "    \n",
    "    \n",
    "\n",
    "    #CONCATENAMOS AMBOS CONJUNTOS PARA APLICAR TRANSFORMACIONES\n",
    "    #Sacamos columna para el clasificador\n",
    "\n",
    "\n",
    "    data=pd.concat([traindata,testdata],axis=0,ignore_index=True) #concatenate training and test set for future transformat\n",
    "\n",
    "    \n",
    "    '''\n",
    "    TRATAMIENTO DE LA COLUMNA Socio_Demo_01\n",
    "        -conversion a variable numerica con mapeo para que los valores de Socio_Demo_01 vayan \n",
    "            de menor a mayor media de poder adquisitivo\n",
    "        -imputacion de valores faltantes\n",
    "        -conversion a variable entera\n",
    "    '''\n",
    "    \n",
    "    #CONVERTIMOS TODOS LOS VALORES EN NUMEROS \n",
    "    mapping=map_order_col_by_target(data,'Socio_Demo_01','Poder_Adquisitivo')\n",
    "    data['Socio_Demo_01']=data['Socio_Demo_01'].map(mapping)\n",
    "    \n",
    "    #RELLENAMOS LOS VALORES FALTANTES DE Socio_Demo_01\n",
    "    data.loc[data['Socio_Demo_01'].isnull(),'Socio_Demo_01']=rellenar_faltantes_random_ponderado(data,'Socio_Demo_01')\n",
    "    \n",
    "    #ESTABA TODAVIA EN FLOAT, LO PASAMOS A INT\n",
    "    data['Socio_Demo_01']=data['Socio_Demo_01'].astype('int')\n",
    "    \n",
    "    '''\n",
    "    FIN DE TRATAMIENTO DE Socio_Demo_01\n",
    "    \n",
    "    \n",
    "    CREACION DE LA COLUMNA CLUSTER\n",
    "        -agrupamos las muestras en 5 clusters y creamos la columna cluster:\n",
    "            muy pobres, pobres, intermedios, ricos, muy ricos\n",
    "        -Mapeamos la columna cluster para que los clusters vayan de menor a mayor media de poder adquisitivo\n",
    "            (Siendo 0 muy pobre y 4 muy rico)\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    kmeans=MiniBatchKMeans(5)\n",
    "    data['cluster']=kmeans.fit_predict(data.drop(['Poder_Adquisitivo','ID_Customer'],axis=1))\n",
    "    mapping=map_order_col_by_target(data,'cluster','Poder_Adquisitivo')\n",
    "    data['cluster']=data['cluster'].map(mapping)\n",
    "    '''\n",
    "    FIN DE CREACION DE LA COLUMNA CLUSTER\n",
    "    '''\n",
    "    \n",
    "    stdSc = StandardScaler()\n",
    "\n",
    "    numeric_cols=numeric_cols[numeric_cols!='Poder_Adquisitivo'] #We don't want to scale SalePrice\n",
    "\n",
    "    data.loc[:, numeric_cols] = stdSc.fit_transform(data.loc[:, numeric_cols])\n",
    "\n",
    "    #SEPARAMOS DE NUEVO LOS CONJUNTOS\n",
    "\n",
    "    traindata=data.iloc[:traindata.shape[0],:] \n",
    "    testdata=data.iloc[traindata.shape[0]:,:]\n",
    "    testdata=testdata.drop('Poder_Adquisitivo',axis=1) #We drop the unknown variable in the test. It was just filled with NAs\n",
    "\n",
    "    traindata_Sin_totales = traindata.drop(['Imp_cons_total','Imp_sal_total','Ind_prod_total','Num_Oper_total','Relacion', \\\n",
    "                                         'Imp_cons_total_2','Imp_sal_total_2','Ind_prod_total_2','Num_Oper_total_2','Relacion_2', \\\n",
    "                                         'Imp_cons_total_3','Imp_sal_total_3','Ind_prod_total_3','Num_Oper_total_3','Relacion_3'],axis=1)\n",
    "\n",
    "    Test_data_Sin_totales = testdata.drop(['Imp_cons_total','Imp_sal_total','Ind_prod_total','Num_Oper_total','Relacion', \\\n",
    "                                         'Imp_cons_total_2','Imp_sal_total_2','Ind_prod_total_2','Num_Oper_total_2','Relacion_2', \\\n",
    "                                         'Imp_cons_total_3','Imp_sal_total_3','Ind_prod_total_3','Num_Oper_total_3','Relacion_3'],axis=1)\n",
    "\n",
    "    traindata_Con_totales = traindata.drop(['Imp_cons_total_2','Imp_sal_total_2','Ind_prod_total_2','Num_Oper_total_2','Relacion_2', \\\n",
    "                                         'Imp_cons_total_3','Imp_sal_total_3','Ind_prod_total_3','Num_Oper_total_3','Relacion_3'],axis=1)\n",
    "\n",
    "    Test_data_Con_totales = testdata.drop(['Imp_cons_total_2','Imp_sal_total_2','Ind_prod_total_2','Num_Oper_total_2','Relacion_2', \\\n",
    "                                         'Imp_cons_total_3','Imp_sal_total_3','Ind_prod_total_3','Num_Oper_total_3','Relacion_3'],axis=1)\n",
    "\n",
    "    traindata_Con_totales_y_cuadrados = traindata.drop(['Imp_cons_total_3','Imp_sal_total_3','Ind_prod_total_3','Num_Oper_total_3','Relacion_3'],axis=1)\n",
    "\n",
    "    Test_data_Con_totales_y_cuadrados = testdata.drop(['Imp_cons_total_3','Imp_sal_total_3','Ind_prod_total_3','Num_Oper_total_3','Relacion_3'],axis=1)\n",
    "    if not os.path.exists('Total'):\n",
    "        os.makedirs('Total')\n",
    "    if not os.path.exists('Sin_Totales'):\n",
    "        os.makedirs('Sin_Totales')\n",
    "    if not os.path.exists('Con_Totales'):\n",
    "        os.makedirs('Con_Totales')\n",
    "    if not os.path.exists('Con_Totales_y_Cuadrados'):\n",
    "        os.makedirs('Con_Totales_y_Cuadrados')\n",
    "        \n",
    "    print('saving files...')    \n",
    "    print('Total')\n",
    "    traindata.to_csv('./Total/traindata.csv', sep=',', encoding='utf-8',index=False)\n",
    "    testdata.to_csv('./Total/TEST.csv', sep=',', encoding='utf-8',index=False)\n",
    "    \n",
    "    print('Sin_Totales')\n",
    "    traindata_Sin_totales.to_csv('./Sin_Totales/traindata.csv', sep=',', encoding='utf-8',index=False)\n",
    "    Test_data_Sin_totales.to_csv('./Sin_Totales/TEST.csv', sep=',', encoding='utf-8',index=False)\n",
    "    \n",
    "    print('Con_Totales')\n",
    "    traindata_Con_totales.to_csv('./Con_Totales/traindata.csv', sep=',', encoding='utf-8',index=False)\n",
    "    Test_data_Con_totales.to_csv('./Con_Totales/TEST.csv', sep=',', encoding='utf-8',index=False)\n",
    "    \n",
    "    print('Con_Totales_y_Cuadrados')\n",
    "    traindata_Con_totales_y_cuadrados.to_csv('./Con_Totales_y_Cuadrados/traindata.csv', sep=',', encoding='utf-8',index=False)\n",
    "    Test_data_Con_totales_y_cuadrados.to_csv('./Con_Totales_y_Cuadrados/TEST.csv', sep=',', encoding='utf-8',index=False)\n",
    "    print('Saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2808: DtypeWarning: Columns (83) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving files...\n",
      "Total\n",
      "Sin_Totales\n",
      "Con_Totales\n",
      "Con_Totales_y_Cuadrados\n",
      "Saved\n"
     ]
    }
   ],
   "source": [
    "do_datasets()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
