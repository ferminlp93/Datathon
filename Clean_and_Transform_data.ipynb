{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "from sklearn.cluster import MiniBatchKMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#RANGO INTERCUARTILICO\n",
    "def iqr_calculate(column):\n",
    "    description = column.describe()\n",
    "    iqr = description['75%']-description['25%']\n",
    "    return iqr\n",
    "#LIMITES VALORES ATIPICOS\n",
    "def outliers_limits(column):\n",
    "    description = column.describe()\n",
    "    iqr = iqr_calculate(column)\n",
    "    outliers_limits = [description['25%']-1.5*iqr,description['75%']+1.5*iqr]\n",
    "    return outliers_limits\n",
    "\n",
    "#LIMITES VALORES ATIPICOS EXTREMOS\n",
    "def extreme_outliers_limits(column):\n",
    "    description = column.describe()\n",
    "    iqr = iqr_calculate(column)\n",
    "    outliers_limits = [description['25%']-3*iqr,description['75%']+3*iqr]\n",
    "    return outliers_limits\n",
    "\n",
    "#ELIMINA VALORES ATIPICOS\n",
    "def delete_outliers(data, column):\n",
    "    limits = outliers_limits(column)\n",
    "    data_without_outliers = data[column<=limits[1]]\n",
    "    data_without_outliers = data_without_outliers[column>=limits[0]]\n",
    "    return data_without_outliers\n",
    "\n",
    "#ELIMINA VALORES ATIPICOS EXTREMOS\n",
    "def delete_extreme_outliers(data, column):\n",
    "    limits = extreme_outliers_limits(column)\n",
    "    data_without_extreme_outliers = data[column<=limits[1]]\n",
    "    data_without_extreme_outliers = data_without_extreme_outliers[column>=limits[0]]\n",
    "    return data_without_extreme_outliers\n",
    "#Agrupa por columnas, saca la media de la columna target para cada grupo y ordena dichos grupos segun la media obtenida\n",
    "def map_order_col_by_target(X,col,target):\n",
    "    \n",
    "    grouped_by_col=X.groupby([col])[target].mean().sort_values()\n",
    "    mapping={}\n",
    "    for i, index in enumerate(grouped_by_col.index):\n",
    "        mapping[index]=i\n",
    "\n",
    "    return mapping\n",
    "\n",
    "#Cantidad de valores faltantes de una o mas (o todas las) columnas\n",
    "def n_missings(X,cols=''):\n",
    "    if cols:\n",
    "        return X[cols].isnull().sum() #solo algunas columnas\n",
    "    else: \n",
    "        return X.isnull().sum() #todas las columnas\n",
    "\n",
    "    \n",
    "#GENERA LOS VALORES FALTANTES DE FORMA ALEATORIA PONDERADA POR LAS FRECUENCIAS RELATIVAS\n",
    "#ES DECIR, CADA VALOR FALTANTE ES MAS PROBABLE QUE SE RELLENE CON LOS MAS FRECUENTES, \n",
    "#ASI SE MANTIENE LA DISTRIBUCION DE PROBABILIDAD ORIGINAL INTACTA    \n",
    "def rellenar_faltantes_random_ponderado(X,col):\n",
    "    #OBTENEMOS LA FRECUENCIA RELATIVA DE CADA VALOR\n",
    "    val_count=X[col].value_counts()/X.shape[0]\n",
    "    \n",
    "    #A LA QUE TIENE MAS FRECUENCIA LE AÑADIMOS LO QUE FALTA PARA LLEGAR A 1(EN LA DIVISION SE PIERDEN DECIMALES Y FALTABA 0.0001 O ASI)\n",
    "    val_count.iloc[0]+=1-val_count.values.sum()\n",
    "    \n",
    "    n_col_nulls=X[col].isnull().sum()\n",
    "    return np.random.choice(val_count.index,n_col_nulls,p=val_count.values)\n",
    "\n",
    "def cargar_originales(train,test):\n",
    "    print('Cargando los datasets originales...')\n",
    "    traindata=pd.read_csv(train)#reading the data\n",
    "    testdata=pd.read_csv(test)\n",
    "    return traindata,testdata\n",
    "\n",
    "def crear_variables(traindata,testdata):\n",
    "    print('Creando Variables para enriquecer el modelo...')\n",
    "    #Ver el nombre de las columnas para saber cuales sumar (buscar la mayor correlación)\n",
    "    list_columns=list(traindata.columns.values)\n",
    "    Names_Imp_cons=list_columns[0:17]\n",
    "    #Importe de consumos habituales del cliente en base a sus operaciones con tarjetas y domiciliaciones más comunes\n",
    "    traindata['Imp_Cons_total'] = traindata[Names_Imp_cons].sum(axis=1)\n",
    "    #Importe de los saldos de los distintos productos financieros.\n",
    "    traindata['Imp_Cons_total_2']=traindata['Imp_Cons_total']**2\n",
    "    traindata['Imp_Cons_total_3']=traindata['Imp_Cons_total']**3\n",
    "    Names_Imp_Sal=list_columns[17:38]\n",
    "    traindata['Imp_Sal_total'] = traindata[Names_Imp_Sal].sum(axis=1)\n",
    "    #Tenencia de los distintos productos financieros. Son indices entre 0 y 2 por lo que el sumatorio también valdra\n",
    "    #ya que cuanta más puntuación saque el sumatorio más productos compra\n",
    "    traindata['Imp_Sal_total_2']=traindata['Imp_Sal_total']**2\n",
    "    traindata['Imp_Sal_total_3']=traindata['Imp_Sal_total']**3\n",
    "    Names_Ind_Prod=list_columns[38:62]\n",
    "    traindata['Ind_Prod_total'] = traindata[Names_Ind_Prod].sum(axis=1)\n",
    "    #Número de operaciones a través de los distintos productos financieros.\n",
    "    traindata['Ind_Prod_total_2'] = traindata['Ind_Prod_total']**2\n",
    "    traindata['Ind_Prod_total_3'] = traindata['Ind_Prod_total']**3\n",
    "    Names_Num_Oper=list_columns[62:82]\n",
    "    traindata['Num_Oper_total'] = traindata[Names_Num_Oper].sum(axis=1)\n",
    "\n",
    "    traindata['Num_Oper_total_2'] = traindata['Num_Oper_total']**2\n",
    "    traindata['Num_Oper_total_3'] = traindata['Num_Oper_total']**3\n",
    "    #datasetfinal\n",
    "    #Se intenta buscar una relación entre el salario consumo y el numero de operaciones\n",
    "    traindata['Relacion']=((traindata.Imp_Sal_total+traindata.Ind_Prod_total)*traindata.Num_Oper_total)-traindata.Imp_Cons_total\n",
    "    \n",
    "\n",
    "    traindata['Relacion_2'] = traindata['Relacion']**2\n",
    "    traindata['Relacion_3'] = traindata['Relacion']**3\n",
    "    #hacemos lo mismo con lo de test (la lista que contiene nombres nos sirve la misma)\n",
    "\n",
    "    testdata['Imp_Cons_total'] = testdata[Names_Imp_cons].sum(axis=1)\n",
    "    testdata['Imp_Cons_total_2'] = testdata['Imp_Cons_total']**2\n",
    "    testdata['Imp_Cons_total_3'] = testdata['Imp_Cons_total']**3\n",
    "    testdata['Imp_Sal_total'] = testdata[Names_Imp_Sal].sum(axis=1)\n",
    "    testdata['Imp_Sal_total_2'] = testdata['Imp_Sal_total']**2\n",
    "    testdata['Imp_Sal_total_3'] = testdata['Imp_Sal_total']**3\n",
    "    testdata['Ind_Prod_total'] = testdata[Names_Ind_Prod].sum(axis=1)\n",
    "    testdata['Ind_Prod_total_2'] = testdata['Ind_Prod_total']**2\n",
    "    testdata['Ind_Prod_total_3'] = testdata['Ind_Prod_total']**3\n",
    "    testdata['Num_Oper_total'] = testdata[Names_Num_Oper].sum(axis=1)\n",
    "    testdata['Num_Oper_total_2'] = testdata['Num_Oper_total']**2\n",
    "    testdata['Num_Oper_total_3'] = testdata['Num_Oper_total']**3\n",
    "    testdata['Relacion']=((testdata.Imp_Sal_total+testdata.Ind_Prod_total)*testdata.Num_Oper_total)-testdata.Imp_Cons_total\n",
    "    testdata['Relacion_2']= testdata['Relacion']**2\n",
    "    testdata['Relacion_3']= testdata['Relacion']**3\n",
    "    \n",
    "def concat_data_trans(traindata,testdata):\n",
    "    print('Concatenando los datasets para el procesado...')\n",
    "    numeric_cols=traindata.select_dtypes(include=[np.number]).columns#select only numerical\n",
    "    nominal_cols=traindata.select_dtypes(exclude=[np.number]).columns#select only non numerical\n",
    "    \n",
    "\n",
    "    #CONCATENAMOS AMBOS CONJUNTOS PARA APLICAR TRANSFORMACIONES\n",
    "    #Sacamos columna para el clasificador\n",
    "\n",
    "    data=pd.concat([traindata,testdata],axis=0,ignore_index=True) #concatenate training and test set for future transformat\n",
    "    return data,numeric_cols,nominal_cols\n",
    "\n",
    "def cat_to_dummy(data):\n",
    "    print('Pasando categoricas a dummies...')\n",
    "    categorical_cols=['Ind_Prod_02', 'Ind_Prod_03',\n",
    "       'Ind_Prod_04', 'Ind_Prod_05', 'Ind_Prod_06', 'Ind_Prod_07',\n",
    "                      'Ind_Prod_09', 'Ind_Prod_10', 'Ind_Prod_11',\n",
    "       'Ind_Prod_12', 'Ind_Prod_13', 'Ind_Prod_14', 'Ind_Prod_15',\n",
    "       'Ind_Prod_16', 'Ind_Prod_18', 'Ind_Prod_19',\n",
    "       'Ind_Prod_20', 'Ind_Prod_21', 'Ind_Prod_22', 'Ind_Prod_23',\n",
    "       'Ind_Prod_24']\n",
    "    \n",
    "    for i in categorical_cols:\n",
    "    \n",
    "        dummy=pd.get_dummies(data[i])\n",
    "        data=pd.concat([data,dummy], axis=1)\n",
    "        \n",
    "    data=data.drop(categorical_cols, axis=1)    \n",
    "    return data\n",
    "\n",
    "\n",
    "def socio_demo(data):\n",
    "    print('Tratando la columna Socio_Demo_01...')\n",
    "    '''\n",
    "    TRATAMIENTO DE LA COLUMNA Socio_Demo_01\n",
    "        -conversion a variable numerica con mapeo para que los valores de Socio_Demo_01 vayan \n",
    "            de menor a mayor media de poder adquisitivo\n",
    "        -imputacion de valores faltantes\n",
    "        -conversion a variable entera\n",
    "    '''\n",
    "    \n",
    "    #CONVERTIMOS TODOS LOS VALORES EN NUMEROS \n",
    "    mapping=map_order_col_by_target(data,'Socio_Demo_01','Poder_Adquisitivo')\n",
    "    data['Socio_Demo_01']=data['Socio_Demo_01'].map(mapping)\n",
    "    \n",
    "    #RELLENAMOS LOS VALORES FALTANTES DE Socio_Demo_01\n",
    "    data.loc[data['Socio_Demo_01'].isnull(),'Socio_Demo_01']=rellenar_faltantes_random_ponderado(data,'Socio_Demo_01')\n",
    "    \n",
    "    #ESTABA TODAVIA EN FLOAT, LO PASAMOS A INT\n",
    "    data['Socio_Demo_01']=data['Socio_Demo_01'].astype('int')\n",
    "    \n",
    "    '''\n",
    "    FIN DE TRATAMIENTO DE Socio_Demo_01\n",
    "    \n",
    "    \n",
    "    CREACION DE LA COLUMNA CLUSTER\n",
    "        -agrupamos las muestras en 5 clusters y creamos la columna cluster:\n",
    "            muy pobres, pobres, intermedios, ricos, muy ricos\n",
    "        -Mapeamos la columna cluster para que los clusters vayan de menor a mayor media de poder adquisitivo\n",
    "            (Siendo 0 muy pobre y 4 muy rico)\n",
    "    '''\n",
    "    return data\n",
    "\n",
    "def create_cluster(data):\n",
    "    print('Creando columna de cluster...')\n",
    "    '''\n",
    "     CREACION DE LA COLUMNA CLUSTER\n",
    "        -agrupamos las muestras en 5 clusters y creamos la columna cluster:\n",
    "            muy pobres, pobres, intermedios, ricos, muy ricos\n",
    "        -Mapeamos la columna cluster para que los clusters vayan de menor a mayor media de poder adquisitivo\n",
    "            (Siendo 0 muy pobre y 4 muy rico)\n",
    "    '''\n",
    "    kmeans=MiniBatchKMeans(5)\n",
    "    data['cluster']=kmeans.fit_predict(data.drop(['Poder_Adquisitivo','ID_Customer'],axis=1))\n",
    "    mapping=map_order_col_by_target(data,'cluster','Poder_Adquisitivo')\n",
    "    data['cluster']=data['cluster'].map(mapping)\n",
    "    '''\n",
    "    FIN DE CREACION DE LA COLUMNA CLUSTER\n",
    "    '''\n",
    "    return data\n",
    "\n",
    "def norm_and_sep(data,traindata,testdata,numeric_cols):\n",
    "    print('Separando los datasets...')\n",
    "    #Normalizar dataset\n",
    "    #stdSc = StandardScaler()\n",
    "\n",
    "    #numeric_cols=numeric_cols[numeric_cols!='Poder_Adquisitivo'] #We don't want to scale SalePrice\n",
    "\n",
    "    #data.loc[:, numeric_cols] = stdSc.fit_transform(data.loc[:, numeric_cols])\n",
    "\n",
    "    #SEPARAMOS DE NUEVO LOS CONJUNTOS\n",
    "\n",
    "    traindata=data.iloc[:traindata.shape[0],:] \n",
    "    testdata=data.iloc[traindata.shape[0]:,:]\n",
    "    testdata=testdata.drop('Poder_Adquisitivo',axis=1) #We drop the unknown variable in the test. It was just filled with NAs\n",
    "    return traindata,testdata\n",
    "\n",
    "def prep_dist_datasets(traindata,testdata):\n",
    "    print('Preparando los datasets para el guardado...')\n",
    "    #Preparamos los datasets con distintos datos para comprobar cual correlaciona mejor en los regresores\n",
    "    traindata_Sin_totales = traindata.drop(['Imp_Cons_total','Imp_Sal_total','Ind_Prod_total','Num_Oper_total','Relacion', \\\n",
    "                                         'Imp_Cons_total_2','Imp_Sal_total_2','Ind_Prod_total_2','Num_Oper_total_2','Relacion_2', \\\n",
    "                                         'Imp_Cons_total_3','Imp_Sal_total_3','Ind_Prod_total_3','Num_Oper_total_3','Relacion_3'],axis=1)\n",
    "\n",
    "    Test_data_Sin_totales = testdata.drop(['Imp_Cons_total','Imp_Sal_total','Ind_Prod_total','Num_Oper_total','Relacion', \\\n",
    "                                         'Imp_Cons_total_2','Imp_Sal_total_2','Ind_Prod_total_2','Num_Oper_total_2','Relacion_2', \\\n",
    "                                         'Imp_Cons_total_3','Imp_Sal_total_3','Ind_Prod_total_3','Num_Oper_total_3','Relacion_3'],axis=1)\n",
    "\n",
    "    traindata_Con_totales = traindata.drop(['Imp_Cons_total_2','Imp_Sal_total_2','Ind_Prod_total_2','Num_Oper_total_2','Relacion_2', \\\n",
    "                                         'Imp_Cons_total_3','Imp_Sal_total_3','Ind_Prod_total_3','Num_Oper_total_3','Relacion_3'],axis=1)\n",
    "\n",
    "    Test_data_Con_totales = testdata.drop(['Imp_Cons_total_2','Imp_Sal_total_2','Ind_Prod_total_2','Num_Oper_total_2','Relacion_2', \\\n",
    "                                         'Imp_Cons_total_3','Imp_Sal_total_3','Ind_Prod_total_3','Num_Oper_total_3','Relacion_3'],axis=1)\n",
    "\n",
    "    traindata_Con_totales_y_cuadrados = traindata.drop(['Imp_Cons_total_3','Imp_Sal_total_3','Ind_Prod_total_3','Num_Oper_total_3','Relacion_3'],axis=1)\n",
    "\n",
    "    \n",
    "    Test_data_Con_totales_y_cuadrados = testdata.drop(['Imp_Cons_total_3','Imp_Sal_total_3','Ind_Prod_total_3','Num_Oper_total_3','Relacion_3'],axis=1)\n",
    "    \n",
    "    Train_sin_out=delete_outliers(traindata,traindata['Poder_Adquisitivo'])\n",
    "    \n",
    "    Train_sin_out_ext=delete_extreme_outliers(traindata,traindata['Poder_Adquisitivo'])\n",
    "    \n",
    "    return traindata_Sin_totales,Test_data_Sin_totales,traindata_Con_totales,\\\n",
    "    Test_data_Con_totales,traindata_Con_totales_y_cuadrados,Test_data_Con_totales_y_cuadrados,\\\n",
    "    Train_sin_out,Train_sin_out_ext\n",
    "\n",
    "\n",
    "def create_folders():\n",
    "    #Comprobamos si existen las carpetas y si no las creamos\n",
    "    print('Creacion de carpetas...')\n",
    "    if not os.path.exists('Total'):\n",
    "        os.makedirs('Total')\n",
    "    if not os.path.exists('Sin_Totales'):\n",
    "        os.makedirs('Sin_Totales')\n",
    "    if not os.path.exists('Con_Totales'):\n",
    "        os.makedirs('Con_Totales')\n",
    "    if not os.path.exists('Con_Totales_y_Cuadrados'):\n",
    "        os.makedirs('Con_Totales_y_Cuadrados')\n",
    "    if not os.path.exists('Total_Sin_Outlayers'):\n",
    "        os.makedirs('Total_Sin_Outlayers')\n",
    "    if not os.path.exists('Total_Sin_Outlayers_Extremos'):\n",
    "        os.makedirs('Total_Sin_Outlayers_Extremos')\n",
    "    if not os.path.exists('Total_Visualizacion'):\n",
    "        os.makedirs('Total_Visualizacion')\n",
    "\n",
    "def save_datasets(traindata,testdata,traindata_Sin_totales,Test_data_Sin_totales,traindata_Con_totales,\\\n",
    "                 Test_data_Con_totales,traindata_Con_totales_y_cuadrados,Test_data_Con_totales_y_cuadrados,\\\n",
    "                 Train_sin_out,Train_sin_out_ext,Train_visu,Test_visu):\n",
    "    #Procedemos a guardar los datasets\n",
    "    print('saving files...')    \n",
    "    print('Total')\n",
    "    traindata.to_csv('./Total/traindata.csv', sep=',', encoding='utf-8',index=False)\n",
    "    testdata.to_csv('./Total/TEST.csv', sep=',', encoding='utf-8',index=False)\n",
    "    \n",
    "    print('Dataset de Visualizacion')\n",
    "    Train_visu.to_csv('./Total_Visualizacion/traindata.csv', sep=',', encoding='utf-8',index=False)\n",
    "    Test_visu.to_csv('./Total_Visualizacion/TEST.csv', sep=',', encoding='utf-8',index=False)\n",
    "    \n",
    "    print('Total sin outlayers')\n",
    "    Train_sin_out.to_csv('./Total_Sin_Outlayers/traindata.csv', sep=',', encoding='utf-8',index=False)\n",
    "    testdata.to_csv('./Total_Sin_Outlayers/TEST.csv', sep=',', encoding='utf-8',index=False)\n",
    "    \n",
    "    print('Total sin outlayers extremos')\n",
    "    Train_sin_out_ext.to_csv('./Total_Sin_Outlayers_Extremos/traindata.csv', sep=',', encoding='utf-8',index=False)\n",
    "    testdata.to_csv('./Total_Sin_Outlayers_Extremos/TEST.csv', sep=',', encoding='utf-8',index=False)\n",
    "    \n",
    "    print('Sin_Totales')\n",
    "    traindata_Sin_totales.to_csv('./Sin_Totales/traindata.csv', sep=',', encoding='utf-8',index=False)\n",
    "    Test_data_Sin_totales.to_csv('./Sin_Totales/TEST.csv', sep=',', encoding='utf-8',index=False)\n",
    "    \n",
    "    print('Con_Totales')\n",
    "    traindata_Con_totales.to_csv('./Con_Totales/traindata.csv', sep=',', encoding='utf-8',index=False)\n",
    "    Test_data_Con_totales.to_csv('./Con_Totales/TEST.csv', sep=',', encoding='utf-8',index=False)\n",
    "    \n",
    "    print('Con_Totales_y_Cuadrados')\n",
    "    traindata_Con_totales_y_cuadrados.to_csv('./Con_Totales_y_Cuadrados/traindata.csv', sep=',', encoding='utf-8',index=False)\n",
    "    Test_data_Con_totales_y_cuadrados.to_csv('./Con_Totales_y_Cuadrados/TEST.csv', sep=',', encoding='utf-8',index=False)\n",
    "    print('Saved')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    #CARGAMOS LOS DATOS\n",
    "    traindata,testdata=cargar_originales('Dataset_Salesforce_Predictive_Modelling_TRAIN.txt', \\\n",
    "                                        'Dataset_Salesforce_Predictive_Modelling_TEST.txt')\n",
    "    \n",
    "    #CREAMOS VARIABLES PARA ENRIQUECER EL MODELO\n",
    "    crear_variables(traindata,testdata)\n",
    "\n",
    "    \n",
    "    #Concat datasets para transformaciones\n",
    "    data,numeric_cols,nominal_cols=concat_data_trans(traindata,testdata)\n",
    "    \n",
    "\n",
    "    #transformacion sociodemo\n",
    "    data=socio_demo(data)\n",
    "    \n",
    "    #Creacion Cluster\n",
    "    data=create_cluster(data)\n",
    "    Train_visu=data\n",
    "    #get dummy\n",
    "    data=cat_to_dummy(data)\n",
    "    \n",
    "\n",
    "    #separarlos en train y test\n",
    "    traindata,testdata = norm_and_sep(data,traindata,testdata,numeric_cols)\n",
    "    traindata_1,testdata_2 = norm_and_sep(Train_visu,traindata,testdata,numeric_cols)\n",
    "    \n",
    "    #Preparar distintos datasets para pruebas\n",
    "    traindata_Sin_totales,Test_data_Sin_totales,traindata_Con_totales,\\\n",
    "    Test_data_Con_totales,traindata_Con_totales_y_cuadrados,Test_data_Con_totales_y_cuadrados,\\\n",
    "    Train_sin_out,Train_sin_out_ext=\\\n",
    "    prep_dist_datasets(traindata,testdata)\n",
    "    \n",
    "    #Comprobar si existen carpetas, y si no crearlas\n",
    "    create_folders()\n",
    "        \n",
    "    #Guardamos los datasets\n",
    "    save_datasets(traindata,testdata,traindata_Sin_totales,Test_data_Sin_totales,traindata_Con_totales,\\\n",
    "                 Test_data_Con_totales,traindata_Con_totales_y_cuadrados,Test_data_Con_totales_y_cuadrados,\\\n",
    "                 Train_sin_out,Train_sin_out_ext,traindata_1,testdata_2)\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando los datasets originales...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fermin/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2881: DtypeWarning: Columns (83) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creando Variables para enriquecer el modelo...\n",
      "Concatenando los datasets para el procesado...\n",
      "Tratando la columna Socio_Demo_01...\n",
      "Creando columna de cluster...\n",
      "Pasando categoricas a dummies...\n",
      "Separando los datasets...\n",
      "Separando los datasets...\n",
      "Preparando los datasets para el guardado...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fermin/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:24: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "/home/fermin/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:31: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creacion de carpetas...\n",
      "saving files...\n",
      "Total\n",
      "Dataset de Visualizacion\n",
      "Total sin outlayers\n",
      "Total sin outlayers extremos\n",
      "Sin_Totales\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
