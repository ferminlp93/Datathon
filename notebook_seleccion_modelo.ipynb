{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, train_test_split,KFold\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.linear_model import Ridge, ElasticNet, Lasso, HuberRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor,GradientBoostingRegressor,AdaBoostRegressor,ExtraTreesRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from bayes_opt import BayesianOptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input: Resultados de una comparacion de modelos\n",
    "#Output: los resultados en forma de tabla\n",
    "def get_models_table(models):\n",
    "    models_table={}\n",
    "    model_names=[get_model_name(model) for model in models]\n",
    "    models_table['Regressor']=model_names\n",
    "    models_table['model']=models\n",
    "    models_table['error']=np.repeat(np.nan,len(models))\n",
    "    models_table['params']=[{} for i in range(len(models))]\n",
    "\n",
    "    return pd.DataFrame(models_table).set_index('Regressor')\n",
    "\n",
    "\n",
    "\n",
    "def remove_outlier_predictions(y_pred,y_train):\n",
    "    min_val, max_val = y_train.min(), y_train.max()\n",
    "    result=y_pred.copy()\n",
    "    result[y_pred<min_val]= min_val\n",
    "    result[y_pred>max_val]= max_val\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "#Input: Un modelo\n",
    "#Output: El nombre del modelo\n",
    "def get_model_name(model):\n",
    "    return str(model.__class__).split('.')[-1].split(\"'\")[0]\n",
    "\n",
    "\n",
    "#Input: Un modelo y un dataset separado en train y test\n",
    "#Output: Obtiene la prediccion del modelo entrenado con el train sobre el conjunto de test\n",
    "def fit_predict(model,X,y,X_test):\n",
    "    model.fit(X,y)\n",
    "    return remove_outlier_predictions(model.predict(X_test),y)\n",
    "\n",
    "\n",
    "#Input: Un modelo y un dataset\n",
    "#Output: Obtiene el error de cross validation del modelo sobre el dataset\n",
    "def error_cv(model,X, y, verbose=0, metric='neg_mean_absolute_error'):\n",
    "    return abs(cross_val_score(model, X, y, scoring = metric, n_jobs=-1, cv=8,verbose=verbose)).mean()\n",
    "\n",
    "\n",
    "#Input: Un modelo, un dataset y un conjunto de parametros\n",
    "#Output: Estima los parametros del modelo con cross validation y devuelve el error de la mejor cmbinacion (y la mejor combinacion)\n",
    "def error_cv_param_grid(model,X, y,param_grid, verbose=0, metric='neg_mean_absolute_error'):\n",
    "    grid_model=GridSearchCV(model,param_grid,scoring=metric,n_jobs=-1,cv=8,refit=False,verbose=verbose).fit(X,y)\n",
    "    return abs(grid_model.best_score_), grid_model.best_params_\n",
    "\n",
    "\n",
    "#Input: Un modelo, un dataset y (opcional) Un grid de parámetros\n",
    "#Output: Devuelve una tabla con la comparativa de los modelos en terminos de error sobre el dataset (y sus parametros optimos si estimate_params=True)\n",
    "def compare_models(models_table, X, y, estimate_params=False, verbose=0, metric='neg_mean_absolute_error'):\n",
    "    \n",
    "    errors=[]\n",
    "    params=[]\n",
    "    for i in range(models_table.shape[0]):\n",
    "        \n",
    "        model=models_table['model'].iloc[i] \n",
    "        if estimate_params:\n",
    "            score,param = error_cv_param_grid(model,X, y,estimate_params[i],verbose=verbose, metric=metric)\n",
    "            params.append(param)\n",
    "        \n",
    "        else:\n",
    "            score=error_cv(models[i],X, y,verbose=verbose ,metric=metric)\n",
    "        \n",
    "        if verbose>0:\n",
    "            print(models_table.index[i],': ',score)\n",
    "\n",
    "        errors.append(score)\n",
    "    \n",
    "    models_table['error']=errors\n",
    "    if estimate_params:\n",
    "        models_table['params']=params\n",
    "        \n",
    "        \n",
    "              \n",
    "class Stacking_model(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, base_models, meta_model, n_folds=5, metric=mean_absolute_error):\n",
    "        self.base_models = base_models\n",
    "        self.meta_model = meta_model\n",
    "        self.n_folds = n_folds\n",
    "        self.metric = metric\n",
    "   \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        X_matrix=X\n",
    "        y_matrix=y\n",
    "        if type(X)== pd.core.frame.DataFrame:\n",
    "            X_matrix=X_matrix.as_matrix()\n",
    "            \n",
    "        if type(y)==pd.core.series.Series:    \n",
    "            y_matrix=y_matrix.as_matrix()\n",
    "        \n",
    "        \n",
    "        self.base_models_ = [list() for x in self.base_models]\n",
    "        self.meta_model_ = clone(self.meta_model)\n",
    "        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n",
    "        \n",
    "        \n",
    "        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n",
    "        for i, model in enumerate(self.base_models):\n",
    "            for train_index, holdout_index in kfold.split(X_matrix, y_matrix):\n",
    "                instance = clone(model)\n",
    "                self.base_models_[i].append(instance)\n",
    "                \n",
    "                instance.fit(X_matrix[train_index], y_matrix[train_index])\n",
    "                y_pred = instance.predict(X_matrix[holdout_index])\n",
    "                out_of_fold_predictions[holdout_index, i] = y_pred\n",
    "                \n",
    "        \n",
    "        self.meta_model_.fit(out_of_fold_predictions, y)\n",
    "        return self\n",
    "   \n",
    "    def predict(self, X_test):\n",
    "        df_test=pd.DataFrame()\n",
    "        for i, base_models in enumerate(self.base_models_):\n",
    "            df_test[i]=np.zeros(X_test.shape[0])\n",
    "            for model in base_models:\n",
    "                df_test[i]+=model.predict(X_test)\n",
    "                \n",
    "            df_test[i]/=len(base_models)\n",
    "        \n",
    "        return self.meta_model_.predict(df_test)\n",
    "    \n",
    "    \n",
    "    def score(self,X_test,y_test):\n",
    "        return self.metric(y_test,self.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows and columns of the training set:  (363801, 104)\n",
      "Number of rows and columns of the test set:  (363801, 104)\n"
     ]
    }
   ],
   "source": [
    "folder='Total'\n",
    "\n",
    "traindata=pd.read_csv(folder+'/traindata.csv')#reading the data\n",
    "testdata=pd.read_csv(folder+'/TEST.csv')#reading the data\n",
    "\n",
    "traindata=traindata.drop('ID_Customer',axis=1)\n",
    "test_ids=testdata['ID_Customer'] #Nos lo guardamos para submision\n",
    "testdata=testdata.drop('ID_Customer',axis=1)\n",
    "\n",
    "traindata=traindata[traindata['Poder_Adquisitivo']<1000000]\n",
    "\n",
    "print('Number of rows and columns of the training set: ',traindata.shape)\n",
    "print('Number of rows and columns of the test set: ',traindata.shape)\n",
    "\n",
    "X_traindata=traindata.drop('Poder_Adquisitivo',axis=1)\n",
    "y_traindata=traindata['Poder_Adquisitivo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models=[\n",
    "    Ridge(),\n",
    "    Lasso(),\n",
    "    ElasticNet(),\n",
    "    KNeighborsRegressor(),\n",
    "    HuberRegressor(),\n",
    "    GradientBoostingRegressor(),\n",
    "    AdaBoostRegressor(),\n",
    "    RandomForestRegressor(),\n",
    "    ExtraTreesRegressor(),\n",
    "    xgb.XGBRegressor()   \n",
    "]\n",
    "\n",
    "\n",
    "models_table=get_models_table(models)\n",
    "\n",
    "    \n",
    "param_grid_list=[\n",
    "    {'alpha':[0.05,0.1,0.3,0.6,1,1.5,3,5,10,15,30,50,80]},\n",
    "    {'alpha':[0.0001, 0.0003, 0.0006, 0.001, 0.003, 0.006, 0.01, 0.03, 0.06, 0.1, 0.3, 0.6, 1.0]},\n",
    "    {'l1_ratio':[0.1, 0.3, 0.5, 0.7, 0.9, 1],'alpha':[0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5]},\n",
    "    {'n_neighbors':[3,5,7,9]},\n",
    "    {'epsilon':[1.0,1.2,1.35,1.5,1.7,2.0], 'alpha':[0.00005,0.0001,0.0003,0.0006,0.0009,0.0012]},\n",
    "    {'learning_rate': [0.1, 0.05, 0.01], 'max_depth': [4, 6, 8, 10, 12], 'n_estimators': range(50, 200, 25)},\n",
    "    {'learning_rate': [0.5, 1, 3,5], 'n_estimators': range(50, 200, 25)},\n",
    "    {'max_features':['auto', 'sqrt', 40], 'max_depth': [4, 6, 8, 10, None], 'n_estimators': range(50, 300, 50)},\n",
    "    {'max_features':['auto', 'sqrt', 40], 'max_depth': [4, 6, 8, 10, None], 'n_estimators': range(50, 300, 50)},\n",
    "    {}\n",
    "]\n",
    "\n",
    "\n",
    "#separamos nuestro conjunto de train en train y validacion\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_traindata, y_traindata, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparamos modelos (cada uno con su mejor combinacion de parametros)\n",
    "compare_models(models_table, X_train, y_train, param_grid_list, verbose=4)\n",
    "\n",
    "#Alternativamente podemos Comparar modelos cada uno con su combinacion de parametros por defecto (mas rapido)\n",
    "#compare_models(models_table, X_traindata, y_traindata, verbose=4)\n",
    "models_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Escogemos un modelo y sus parametros en base a los resultados obtenidos arriba\n",
    "#Validamos el modelo obteniendo el error para el conjunto de test\n",
    "key=models_table['error'].argmin()\n",
    "\n",
    "best_model=models_table.loc[key,'model'].set_params(**models_table.loc[key,'params'])\n",
    "print('Validation mean absolute error: ',mean_absolute_error(y_val,fit_predict(best_model,X_train,y_train,X_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_model.fit(X_traindata, y_traindata)\n",
    "\n",
    "submision=pd.DataFrame()\n",
    "submision['ID_Customer']=test_ids\n",
    "submision['PA_Est']=fit_predict(best_model,X_traindata,y_traindata,testdata)\n",
    "print('The description of the submision:\\n',submision.describe())\n",
    "submision.to_csv('Test_Mission.txt',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# De aquí para abajo sin pruebas, así que ni caso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred6=fit_predict(Stacking_model([Ridge(),Ridge()],Ridge()),X_train,y_train,X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5185.6150812617389"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(y_val,y_pred6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  ................................................................\n",
      "[CV]  ................................................................\n",
      "[CV]  ................................................................\n",
      "[CV]  ................................................................\n",
      "[CV] ....................... , score=-5740.981967166104, total=  17.8s\n",
      "[CV]  ................................................................\n",
      "[CV] ....................... , score=-5323.258109960747, total=  18.4s\n",
      "[CV]  ................................................................\n",
      "[CV] ....................... , score=-5442.497596179486, total=  19.4s\n",
      "[CV]  ................................................................\n",
      "[CV] ....................... , score=-4942.859665523144, total=  21.1s\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   4 out of   8 | elapsed:   24.0s remaining:   24.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ....................... , score=-4789.555751520681, total=  19.2s\n",
      "[CV] ....................... , score=-5088.686880906654, total=  20.6s\n",
      "[CV] ........................ , score=-5514.05504801813, total=  20.0s\n",
      "[CV] ....................... , score=-5059.818126251833, total=  18.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   8 out of   8 | elapsed:   43.0s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5237.7141431908476"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_cv(Stacking_model([Ridge(),Ridge()],Ridge()),X_traindata,y_traindata,verbose=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred=fit_predict(GradientBoostingRegressor(n_estimators=3000,loss='huber'),X_train,y_train,X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred2=fit_predict(xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n",
    "                             learning_rate=0.05, max_depth=3, \n",
    "                             min_child_weight=1.7817, n_estimators=2200,\n",
    "                             reg_alpha=0.4640, reg_lambda=0.8571,\n",
    "                             subsample=0.5213, silent=1,\n",
    "                             random_state =7, nthread = -1),X_train,y_train,X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred3=fit_predict(lgb.LGBMRegressor(objective='regression',num_leaves=5,\n",
    "                              learning_rate=0.05, n_estimators=720,\n",
    "                              max_bin = 55, bagging_fraction = 0.8,\n",
    "                              bagging_freq = 5, feature_fraction = 0.2319,\n",
    "                              feature_fraction_seed=9, bagging_seed=9,\n",
    "                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11),X_train,y_train,X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_error(y_val,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_error(y_val,y_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_error(y_val,y_pred3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mean_absolute_error(y_val,y_pred4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_cv(xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n",
    "                             learning_rate=0.05, max_depth=3, \n",
    "                             min_child_weight=1.7817, n_estimators=2200,\n",
    "                             reg_alpha=0.4640, reg_lambda=0.8571,\n",
    "                             subsample=0.5213, silent=1,\n",
    "                             random_state =7, nthread = -1),X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "        'silent': True,\n",
    "        'eval_metric': 'mae',\n",
    "        'verbose_eval': True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def xgb_evaluate(min_child_weight,\n",
    "                 colsample_bytree,\n",
    "                 max_depth,\n",
    "                 subsample,\n",
    "                 gamma,\n",
    "                 alpha):\n",
    "\n",
    "    params['min_child_weight'] = int(min_child_weight)\n",
    "    params['cosample_bytree'] = max(min(colsample_bytree, 1), 0)\n",
    "    params['max_depth'] = int(max_depth)\n",
    "    params['subsample'] = max(min(subsample, 1), 0)\n",
    "    params['gamma'] = max(gamma, 0)\n",
    "    params['alpha'] = max(alpha, 0)\n",
    "\n",
    "    return -mae_cv(xgb.XGBRegressor(**params),X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-25-882cd3945410>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-25-882cd3945410>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    'min_child_weight': (1, 20),\u001b[0m\n\u001b[0m                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "xgbBO = BayesianOptimization(xgb_evaluate, {'n_estimators':(100,3000),\n",
    "                                            'learning_rate':(0.05,0.5),\n",
    "                                            'min_child_weight': (1, 20),\n",
    "                                            'colsample_bytree': (0.1, 1),\n",
    "                                            'max_depth': (5, 15),\n",
    "                                            'subsample': (0.5, 1),\n",
    "                                            'gamma': (0, 10),\n",
    "                                            'alpha': (0, 10),\n",
    "                                            })\n",
    "\n",
    "xgbBO.maximize(init_points=5, n_iter=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
