{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, train_test_split,KFold\n",
    "from sklearn.metrics import mean_absolute_error, make_scorer\n",
    "from sklearn.linear_model import Ridge, ElasticNet, Lasso, HuberRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor,GradientBoostingRegressor,ExtraTreesRegressor\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from bayes_opt import BayesianOptimization\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Input: Resultados de una comparacion de modelos\n",
    "#Output: los resultados en forma de tabla\n",
    "def get_models_table(models):\n",
    "    models_table={}\n",
    "    model_names=[get_model_name(model) for model in models]\n",
    "    models_table['Regressor']=model_names\n",
    "    models_table['model']=models\n",
    "    models_table['error']=np.repeat(np.nan,len(models))\n",
    "    models_table['params']=[{} for i in range(len(models))]\n",
    "\n",
    "    return pd.DataFrame(models_table).set_index('Regressor')\n",
    "\n",
    "\n",
    "\n",
    "def remove_outlier_predictions(y_pred,y_train):\n",
    "    min_val, max_val = y_train.min(), y_train.max()\n",
    "    result=y_pred.copy()\n",
    "    result[y_pred<min_val]= min_val\n",
    "    result[y_pred>max_val]= max_val\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "#Input: Un modelo\n",
    "#Output: El nombre del modelo\n",
    "def get_model_name(model):\n",
    "    if 'pipeline' in str(type(model)):\n",
    "        return model.steps[1][0]\n",
    "    else:\n",
    "        return str(model.__class__).split('.')[-1].split(\"'\")[0]\n",
    "\n",
    "\n",
    "#Input: Un modelo y un dataset separado en train y test\n",
    "#Output: Obtiene la prediccion del modelo entrenado con el train sobre el conjunto de test\n",
    "def fit_predict(model,X,y,X_test):\n",
    "    model.fit(X,y)\n",
    "    return remove_outlier_predictions(model.predict(X_test),y)\n",
    "\n",
    "\n",
    "#Input: Un modelo y un dataset\n",
    "#Output: Obtiene el error de cross validation del modelo sobre el dataset\n",
    "def error_cv(model,X, y, verbose=0, metric='neg_mean_absolute_error', cv=8):\n",
    "    return abs(cross_val_score(model, X, y, scoring = metric, n_jobs=-1, cv=cv,verbose=verbose)).mean()\n",
    "\n",
    "def cast_to_int(params):\n",
    "    result={}\n",
    "    for key in set(['n_estimators','min_samples_split','max_depth','max_bin','num_leaves','min_data_in_leaf']).intersection(set(params.keys())):\n",
    "        result[key]=int(params[key])\n",
    "    return result\n",
    "\n",
    "def model_evaluate(model,X,y,cv,**params):\n",
    "    return -error_cv(model(**cast_to_int(params)),X,y,cv=cv)\n",
    "\n",
    "#Input: Un modelo, un dataset y un conjunto de parametros\n",
    "#Output: Estima los parametros del modelo con cross validation y devuelve el error de la mejor cmbinacion (y la mejor combinacion)\n",
    "def error_cv_param_grid(model,X, y,param_grid, verbose=0, metric='neg_mean_absolute_error', cv=8):\n",
    "    modeltype=str(type(model))\n",
    "    if 'ensemble' in modeltype or 'xgboost' in modeltype or 'lightgbm' in modeltype:\n",
    "        rfcBO = BayesianOptimization(partial(model_evaluate,model=type(model),X=X,y=y,cv=cv),param_grid)\n",
    "        rfcBO.maximize(n_iter=1)\n",
    "        return abs(rfcBO.res['max']['max_val']),rfcBO.res['max']['max_params']\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        grid_model=GridSearchCV(model,param_grid,scoring=metric,n_jobs=-1,cv=cv,refit=False,verbose=verbose).fit(X,y)\n",
    "        return abs(grid_model.best_score_), grid_model.best_params_\n",
    "\n",
    "\n",
    "#Input: Un modelo, un dataset y (opcional) Un grid de parÃ¡metros\n",
    "#Output: Devuelve una tabla con la comparativa de los modelos en terminos de error sobre el dataset (y sus parametros optimos si estimate_params=True)\n",
    "def compare_models(models_table, X, y, estimate_params=False, verbose=0, metric='neg_mean_absolute_error', cv=8):\n",
    "    \n",
    "    errors=[]\n",
    "    params=[]\n",
    "    for i in range(models_table.shape[0]):\n",
    "        \n",
    "        model=models_table['model'].iloc[i] \n",
    "        if estimate_params:\n",
    "            score,param = error_cv_param_grid(model,X, y,estimate_params[i],verbose=verbose, metric=metric, cv=cv)\n",
    "            params.append(param)\n",
    "        \n",
    "        else:\n",
    "            score=error_cv(models[i],X, y,verbose=verbose ,metric=metric, cv=cv)\n",
    "        \n",
    "        if verbose>0:\n",
    "            print(models_table.index[i],': ',score)\n",
    "\n",
    "        errors.append(score)\n",
    "    \n",
    "    models_table['error']=errors\n",
    "    if estimate_params:\n",
    "        models_table['params']=params\n",
    "        \n",
    "         \n",
    "class Stacking_model(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, base_models, meta_model, n_folds=5, metric=mean_absolute_error):\n",
    "        self.base_models = base_models\n",
    "        self.meta_model = meta_model\n",
    "        self.n_folds = n_folds\n",
    "        self.metric = metric\n",
    "   \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        X_matrix=X\n",
    "        y_matrix=y\n",
    "        if type(X)== pd.core.frame.DataFrame:\n",
    "            X_matrix=X_matrix.as_matrix()\n",
    "            \n",
    "        if type(y)==pd.core.series.Series:    \n",
    "            y_matrix=y_matrix.as_matrix()\n",
    "        \n",
    "        \n",
    "        self.base_models_ = [list() for x in self.base_models]\n",
    "        self.meta_model_ = clone(self.meta_model)\n",
    "        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n",
    "        \n",
    "        \n",
    "        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n",
    "        for i, model in enumerate(self.base_models):\n",
    "            for train_index, holdout_index in kfold.split(X_matrix, y_matrix):\n",
    "                instance = clone(model)\n",
    "                self.base_models_[i].append(instance)\n",
    "                \n",
    "                instance.fit(X_matrix[train_index], y_matrix[train_index])\n",
    "                y_pred = instance.predict(X_matrix[holdout_index])\n",
    "                out_of_fold_predictions[holdout_index, i] = y_pred\n",
    "                \n",
    "        \n",
    "        self.meta_model_.fit(out_of_fold_predictions, y)\n",
    "        return self\n",
    "   \n",
    "    def predict(self, X_test):\n",
    "        df_test=pd.DataFrame()\n",
    "        for i, base_models in enumerate(self.base_models_):\n",
    "            df_test[i]=np.zeros(X_test.shape[0])\n",
    "            for model in base_models:\n",
    "                df_test[i]+=model.predict(X_test)\n",
    "                \n",
    "            df_test[i]/=len(base_models)\n",
    "        \n",
    "        return self.meta_model_.predict(df_test)\n",
    "    \n",
    "    \n",
    "    def score(self,X_test,y_test):\n",
    "        return self.metric(y_test,self.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "folder='Total'\n",
    "\n",
    "traindata=pd.read_csv(folder+'/traindata.csv')#reading the data\n",
    "testdata=pd.read_csv(folder+'/TEST.csv')#reading the data\n",
    "\n",
    "traindata=traindata.drop('ID_Customer',axis=1)\n",
    "test_ids=testdata['ID_Customer'] #Nos lo guardamos para submision\n",
    "testdata=testdata.drop('ID_Customer',axis=1)\n",
    "\n",
    "traindata=traindata[traindata['Poder_Adquisitivo']<1000000]\n",
    "\n",
    "print('Number of rows and columns of the training set: ',traindata.shape)\n",
    "print('Number of rows and columns of the test set: ',traindata.shape)\n",
    "\n",
    "X_traindata=traindata.drop('Poder_Adquisitivo',axis=1)\n",
    "y_traindata=traindata['Poder_Adquisitivo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models=[\n",
    "    make_pipeline(RobustScaler(), Ridge()),\n",
    "    make_pipeline(RobustScaler(), Lasso()),\n",
    "    make_pipeline(RobustScaler(), ElasticNet()),\n",
    "    HuberRegressor(),\n",
    "    RandomForestRegressor(),\n",
    "    ExtraTreesRegressor(),\n",
    "    GradientBoostingRegressor(loss='huber'),\n",
    "    xgb.XGBRegressor(),\n",
    "    lgb.LGBMRegressor(objective='huber')\n",
    "]\n",
    "\n",
    "\n",
    "models_table=get_models_table(models)\n",
    "\n",
    "    \n",
    "param_grid_list=[\n",
    "    #LINEAR MODELS\n",
    "    {'ridge__alpha':[0.05,0.1,0.3,0.6,1,1.5,3,5,10,15,30,50,80,100]},\n",
    "    {'lasso__alpha':[0.0001,0.0003,0.0006,0.001,0.003,0.006,0.01,0.03,0.06,0.1,0.3,0.6,1.0]},\n",
    "    {'elasticnet__l1_ratio':[0.1,0.3,0.6,0.9,1],'elasticnet__alpha':[0.001,0.005,0.01,0.05,0.1,0.5,1]},\n",
    "    {'epsilon':[1.0,1.2,1.35,1.5,1.7,2.0], 'alpha':[0.00005,0.0001,0.0003,0.0006,0.0009,0.0012]},\n",
    "    \n",
    "    #ENSEMBLE MODELS\n",
    "    {'n_estimators': (10, 300),'min_samples_split': (2, 25),'max_features': (0.1, 0.999),'max_depth': (4,12)},\n",
    "    {'n_estimators': (10, 300),'min_samples_split': (2, 25),'max_features': (0.1, 0.999),'max_depth': (4,12)},\n",
    "    \n",
    "    {'n_estimators':(100,3000),'learning_rate':(0.05,0.5),'subsample':(0.5,1),'max_depth':(5,15),\n",
    "     'min_samples_leaf':(5, 20),'min_samples_split':(2, 12),'alpha':(0,1.5)},\n",
    "    \n",
    "    {'n_estimators':(100,3000),'learning_rate':(0.05,0.5),'subsample':(0.5,1),'max_depth':(5,15),'reg_alpha':(0,1.4),\n",
    "     'reg_lambda':(0,1.4),'min_child_weight':(1,10),'colsample_bytree':(0.1,1),'gamma':(0,1.4)},\n",
    "    \n",
    "    {'n_estimators':(100,3000),'learning_rate':(0.005,0.1),'subsample':(0.5,1),'max_depth':(5,15),'reg_alpha':(0,1.4),\n",
    "     'reg_lambda':(0,1.4),'colsample_bytree':(0.6,0.8),'max_bin':(128,512),'num_leaves':(2,32),'min_data_in_leaf':(20,200)}\n",
    "]\n",
    "\n",
    "\n",
    "#separamos nuestro conjunto de train en train y validacion\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_traindata, y_traindata, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Comparamos modelos (cada uno con su mejor combinacion de parametros)\n",
    "compare_models(models_table, X_train, y_train, param_grid_list, verbose=1, cv=4)\n",
    "\n",
    "#Alternativamente podemos Comparar modelos cada uno con su combinacion de parametros por defecto (mas rapido)\n",
    "#compare_models(models_table, X_traindata, y_traindata, verbose=1, cv=4)#, metric=scorer)\n",
    "models_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejecuta todo hasta aqui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Escogemos un modelo y sus parametros en base a los resultados obtenidos arriba\n",
    "#Validamos el modelo obteniendo el error para el conjunto de test\n",
    "key=models_table['error'].argmin()\n",
    "\n",
    "best_model=models_table.loc[key,'model'].set_params(**models_table.loc[key,'params'])\n",
    "print('Validation mean absolute error: ',mean_absolute_error(y_val,fit_predict(best_model,X_train,y_train,X_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_model.fit(X_traindata, y_traindata)\n",
    "\n",
    "submision=pd.DataFrame()\n",
    "submision['ID_Customer']=test_ids\n",
    "submision['PA_Est']=fit_predict(best_model,X_traindata,y_traindata,testdata)\n",
    "print('The description of the submision:\\n',submision.describe())\n",
    "submision.to_csv('Test_Mission.txt',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred6=fit_predict(Stacking_model([Ridge(),Ridge()],Ridge()),X_train,y_train,X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred6=transformacion_exponencial(y_pred6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mean_absolute_error(y_val,y_pred6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "error_cv(Stacking_model([Ridge(),Ridge()],Ridge()),X_traindata,y_traindata,verbose=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred=fit_predict(GradientBoostingRegressor(n_estimators=3000,loss='huber'),X_train,y_train,X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred2=fit_predict(xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n",
    "                             learning_rate=0.05, max_depth=3, \n",
    "                             min_child_weight=1.7817, n_estimators=2200,\n",
    "                             reg_alpha=0.4640, reg_lambda=0.8571,\n",
    "                             subsample=0.5213, silent=1,\n",
    "                             random_state =7, nthread = -1),X_train,y_train,X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred3=fit_predict(lgb.LGBMRegressor(objective='regression',num_leaves=5,\n",
    "                              learning_rate=0.05, n_estimators=720,\n",
    "                              max_bin = 55, bagging_fraction = 0.8,\n",
    "                              bagging_freq = 5, feature_fraction = 0.2319,\n",
    "                              feature_fraction_seed=9, bagging_seed=9,\n",
    "                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11),X_train,y_train,X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mean_absolute_error(y_val,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mean_absolute_error(y_val,y_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mean_absolute_error(y_val,y_pred3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mean_absolute_error(y_val,y_pred4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mae_cv(xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n",
    "                             learning_rate=0.05, max_depth=3, \n",
    "                             min_child_weight=1.7817, n_estimators=2200,\n",
    "                             reg_alpha=0.4640, reg_lambda=0.8571,\n",
    "                             subsample=0.5213, silent=1,\n",
    "                             random_state =7, nthread = -1),X_train,y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
